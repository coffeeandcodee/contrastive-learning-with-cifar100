{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "327b61ee",
   "metadata": {},
   "source": [
    "# ðŸ“Š **COMPARING AND CONTRASTING VISION MODELS** ðŸ“Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef8e279b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading CIFAR-100 vocabulary...\n",
      "âœ“ CIFAR-100 vocabulary loaded: 100 classes\n",
      "=== REASSESSING v5 ===\n",
      "Loaded from epoch: 38\n",
      "Val Similarity: 0.4199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting embeddings:   0%|          | 0/40 [00:00<?, ?it/s]/Users/voodoo/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/voodoo/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "Collecting embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:19<00:00,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Per-Class Similarity Analysis:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Top 10 Best Aligned Classes:\n",
      " 1. bicycle         | Mean: 0.6283 Â± 0.1384\n",
      " 2. fox             | Mean: 0.6136 Â± 0.0656\n",
      " 3. rabbit          | Mean: 0.6095 Â± 0.0624\n",
      " 4. wolf            | Mean: 0.6065 Â± 0.0596\n",
      " 5. lion            | Mean: 0.5972 Â± 0.0834\n",
      " 6. leopard         | Mean: 0.5854 Â± 0.0847\n",
      " 7. seal            | Mean: 0.5836 Â± 0.0521\n",
      " 8. tiger           | Mean: 0.5776 Â± 0.0974\n",
      " 9. snake           | Mean: 0.5740 Â± 0.0880\n",
      "10. spider          | Mean: 0.5733 Â± 0.0745\n",
      "\n",
      "Bottom 10 Worst Aligned Classes:\n",
      " 1. can             | Mean: 0.3315 Â± 0.1440\n",
      " 2. mountain        | Mean: 0.3262 Â± 0.1898\n",
      " 3. tank            | Mean: 0.3237 Â± 0.1439\n",
      " 4. skyscraper      | Mean: 0.3233 Â± 0.1746\n",
      " 5. baby            | Mean: 0.3144 Â± 0.1366\n",
      " 6. palm_tree       | Mean: 0.3065 Â± 0.1884\n",
      " 7. willow_tree     | Mean: 0.3020 Â± 0.1601\n",
      " 8. lamp            | Mean: 0.2896 Â± 0.1973\n",
      " 9. ray             | Mean: 0.2326 Â± 0.1342\n",
      "10. mouse           | Mean: 0.2147 Â± 0.0732\n",
      "\n",
      "ðŸ“Š Retrieval Performance:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Image-to-Text Retrieval (Recall@K):\n",
      "  Recall@ 1: 32.47% (3247/10000)\n",
      "  Recall@ 5: 52.04% (5204/10000)\n",
      "  Recall@10: 60.93% (6093/10000)\n",
      "\n",
      "Text-to-Image Retrieval (Recall@K):\n",
      "  Recall@ 1: 51.00% (51/100)\n",
      "  Recall@ 5: 71.00% (71/100)\n",
      "  Recall@10: 89.00% (89/100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/voodoo/Library/Python/3.9/lib/python/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/voodoo/Library/Python/3.9/lib/python/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/voodoo/Library/Python/3.9/lib/python/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    }
   ],
   "source": [
    "# REASSESS v5 (with correct architecture)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "Q6_PATH = '/Users/voodoo/Documents/Artificial Intelligence/Q6'\n",
    "if Q6_PATH not in sys.path:\n",
    "    sys.path.insert(0, Q6_PATH)\n",
    "\n",
    "from lab7 import create_mappings, get_cifar100_vocabulary\n",
    "from lab8 import (\n",
    "    CIFAR100Filtered, filter_dataset_indices,\n",
    "    create_data_splits, create_dataloaders, collect_embeddings,\n",
    "    compute_alignment_metrics, print_analysis_results\n",
    ")\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# Custom ImageEncoder with v5 architecture (LARGE head)\n",
    "class ImageEncoderLarge(nn.Module):\n",
    "    def __init__(self, proj_dim=32, device=\"mps\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        base = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n",
    "        self.backbone = nn.Sequential(*list(base.children())[:-1]).to(device).eval()\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        # LARGE projection head (v5)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(576, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, proj_dim)\n",
    "        ).to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            feats = self.backbone(x).flatten(1)\n",
    "        out = self.projection(feats)\n",
    "        return feats, out\n",
    "\n",
    "# Load text embeddings\n",
    "checkpoint_text = torch.load('EMB32_NG10_CS2_BS64.pth', map_location='cpu')\n",
    "nodes = checkpoint_text['nodes']\n",
    "embeddings = checkpoint_text['model_state_dict']['center_embeddings.weight'].numpy()\n",
    "word_to_idx, idx_to_word = create_mappings(nodes)\n",
    "\n",
    "# Get CIFAR vocab and create mappings\n",
    "cifar_vocab = get_cifar100_vocabulary()\n",
    "class_words = [w for w in cifar_vocab if w in word_to_idx]\n",
    "label_to_word = {i: word for i, word in enumerate(cifar_vocab)}\n",
    "label_to_emb_idx = {i: word_to_idx[word] for i, word in enumerate(cifar_vocab) if word in word_to_idx}\n",
    "\n",
    "# Create text embeddings tensor\n",
    "training_text_emb = torch.tensor(\n",
    "    [embeddings[word_to_idx[word]] for word in class_words],\n",
    "    dtype=torch.float32\n",
    ").to(device)\n",
    "training_text_emb = F.normalize(training_text_emb, p=2, dim=1)\n",
    "\n",
    "# Create dataloaders\n",
    "train_full = CIFAR100Filtered(root=\"./data\", split=\"train\")\n",
    "test_full = CIFAR100Filtered(root=\"./data\", split=\"val\")\n",
    "all_train_idx = filter_dataset_indices(train_full, label_to_emb_idx)\n",
    "test_idx = filter_dataset_indices(test_full, label_to_emb_idx)\n",
    "train_idx, val_idx = create_data_splits(all_train_idx, val_ratio=0.2, seed=42)\n",
    "dataloaders = create_dataloaders(train_idx, val_idx, test_idx, {'train': 1024, 'eval': 256})\n",
    "\n",
    "# Load and evaluate v5 with LARGE architecture\n",
    "checkpoint_v5 = torch.load('new_config_clip_vision_model_v5.pth', map_location=device)\n",
    "\n",
    "print(\"=== REASSESSING v5 ===\")\n",
    "print(f\"Loaded from epoch: {checkpoint_v5['epoch']}\")\n",
    "print(f\"Val Similarity: {checkpoint_v5['val_similarity']:.4f}\")\n",
    "\n",
    "vision_model_v5 = ImageEncoderLarge(proj_dim=32, device=device)\n",
    "vision_model_v5.load_state_dict(checkpoint_v5['model_state_dict'], strict=False)\n",
    "vision_model_v5.eval()\n",
    "\n",
    "visual_emb_v5, all_labels_v5 = collect_embeddings(vision_model_v5, dataloaders['test'], device)\n",
    "\n",
    "class_stats_v5, i2t_recalls_v5, t2i_recalls_v5, sim_matrix_v5 = compute_alignment_metrics(\n",
    "    visual_emb_v5, all_labels_v5, training_text_emb.cpu().numpy(), class_words, label_to_word\n",
    ")\n",
    "\n",
    "print_analysis_results(class_stats_v5, i2t_recalls_v5, t2i_recalls_v5, len(all_labels_v5), len(class_words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
